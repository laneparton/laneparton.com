{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "- Goal is to programmatically find the solution to minimum of any cost function\n",
    "- J may have more than one minimum (not just a simple curve)\n",
    "- Repeat until convergence\n",
    "- Simulataneous update of w and b - don't update w before updating b (store via temp variables)\n",
    "- TODO: Note about positive/negative values\n",
    "\n",
    "- If learning rate is too small, gradient descent is slow\n",
    "- If learning rate is too large, gradient descent may never reach the minimum (it will fail to converge)\n",
    "- What if you start at a local minimum?\n",
    "  - w = w\n",
    "  - Gradient descent leaves it unchanged\n",
    "- Can reach local minimum with a fixed learning rate - gradient descent will take small (learning rate) steps near a local minimum because the derivative gets smaller\n",
    "- Convex = one global minimum, easy to get to so long as learning rate is appropriate\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
